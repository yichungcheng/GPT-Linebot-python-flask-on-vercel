{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBCX2Y2OQ5tw4GIb7qZoDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yichungcheng/GPT-Linebot-python-flask-on-vercel/blob/master/00_SSLMSM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1mm74P0RYxUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-supervised Learning for Endoscopic Video Analysis"
      ],
      "metadata": {
        "id": "4X3UrjsG7QxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 設置與安裝函式庫\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q segmentation_models_pytorch\n",
        "!pip install -q albumentations # 數據增強庫\n",
        "!pip install -q pycocotools # 處理 COCO 格式的官方工具，用於解析多邊形\n",
        "!pip install tensorflow\n",
        "\n",
        "# 2. 匯入必要的函式庫\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from pycocotools.coco import COCO\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# 3. 連接 Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 4. 定義路徑\n",
        "# 您的資料夾名稱\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "# DATASET_NAME = 'Endoscope Spine Surgery.v12i.coco'\n",
        "# UNZIP_PATH = '/content/dataset'\n",
        "\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # 假設數據已經在 Drive 資料夾內\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# 檢查路徑結構\n",
        "print(f\"數據根目錄: {DATA_ROOT}\")"
      ],
      "metadata": {
        "id": "3TYt5pXVID_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e32f25f-9438-40f7-f402-67651f895017"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Mounted at /content/gdrive\n",
            "數據根目錄: /content/gdrive/MyDrive/PMBM/論文程式/MSM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ssl_msn/\n",
        "##### ├── main.py\n",
        "##### ├── dataset.py\n",
        "##### ├── augment.py\n",
        "##### ├── model.py\n",
        "##### ├── loss.py\n",
        "##### └── train.py"
      ],
      "metadata": {
        "id": "wnt5a8jw7fYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset.py"
      ],
      "metadata": {
        "id": "qmD0uD1h79qN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YZYLjMkv7P31"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# import os\n",
        "\n",
        "# IMG_SIZE = 224\n",
        "\n",
        "# def parse_image(path):\n",
        "#     img = tf.io.read_file(path)\n",
        "#     img = tf.image.decode_jpeg(img, channels=3)\n",
        "#     img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "#     img = tf.cast(img, tf.float32) / 255.0\n",
        "#     return img\n",
        "\n",
        "# def build_dataset(frames_root, batch_size):\n",
        "#     \"\"\"\n",
        "#     frames_root/\n",
        "#       ├── Video01/*.jpg\n",
        "#       ├── Video02/*.jpg\n",
        "#       └── ...\n",
        "#     \"\"\"\n",
        "\n",
        "#     all_image_paths = []\n",
        "\n",
        "#     # Iterate through each video directory from Video01 to Video80\n",
        "#     for i in range(1, 81):\n",
        "#         video_dir = os.path.join(frames_root, f\"Video{i:02d}\")\n",
        "#         if not os.path.isdir(video_dir):\n",
        "#             print(f\"Warning: Video directory not found: {video_dir}\")\n",
        "#             continue\n",
        "\n",
        "#         # List all image files within the current video directory\n",
        "#         for filename in os.listdir(video_dir):\n",
        "#             if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "#                 all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "#     print(f\"[INFO] Total frames loaded: {len(all_image_paths)}\")\n",
        "\n",
        "#     ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "#     ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "#     ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "#     ds = ds.batch(batch_size)\n",
        "#     ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#     return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "三、Strong Augmentation（MSN 的靈魂）\n",
        "augment.py四、MSN View Generator（same image → 2 views）"
      ],
      "metadata": {
        "id": "wz92f9Q48mT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# def strong_augment(x):\n",
        "#     x = tf.image.random_flip_left_right(x)\n",
        "#     x = tf.image.random_brightness(x, 0.4)\n",
        "#     x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "#     x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "#     # random crop\n",
        "#     crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "#     x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "#     x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "#     return x\n",
        "# def two_views(batch):\n",
        "#     v1 = tf.map_fn(strong_augment, batch)\n",
        "#     v2 = tf.map_fn(strong_augment, batch)\n",
        "#     return v1, v2\n"
      ],
      "metadata": {
        "id": "yb3lRp8E8moQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "五、Backbone + Projection Head\n",
        "model.py"
      ],
      "metadata": {
        "id": "EvN6KJzr8xFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models\n",
        "# def build_backbone():\n",
        "#     base = tf.keras.applications.ResNet50(\n",
        "#         include_top=False,\n",
        "#         weights=None,\n",
        "#         input_shape=(224, 224, 3),\n",
        "#         pooling=\"avg\"\n",
        "#     )\n",
        "#     return base\n",
        "# def projection_head(dim=256):\n",
        "#     return models.Sequential([\n",
        "#         layers.Dense(1024, activation=\"gelu\"),\n",
        "#         layers.Dense(dim)\n",
        "#     ])\n"
      ],
      "metadata": {
        "id": "ViAz2dsM8xcd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "六、MSN Loss（Student → Teacher distribution）\n",
        "loss.py"
      ],
      "metadata": {
        "id": "pOKECRGJ9IL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# def msn_loss(student, teacher, temperature=0.1):\n",
        "#     student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "#     teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "#     return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n"
      ],
      "metadata": {
        "id": "sNM1IyHc9Ibh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "七、EMA Teacher Update（核心）"
      ],
      "metadata": {
        "id": "JoPgr0oU9QPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def update_teacher(student, teacher, momentum=0.996):\n",
        "#     for s, t in zip(student.trainable_variables,\n",
        "#                     teacher.trainable_variables):\n",
        "#         t.assign(momentum * t + (1.0 - momentum) * s)\n"
      ],
      "metadata": {
        "id": "VA50va8s9QvO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "八、Training Step（tf.function）\n",
        "train.py"
      ],
      "metadata": {
        "id": "nlkz3mSg9S3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is now redundant as train_step is defined in train.py and imported by the main script."
      ],
      "metadata": {
        "id": "rs_u7RPz9TJM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "九、Main（完整可跑）"
      ],
      "metadata": {
        "id": "v6cV2YQV9a97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    return tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=tf.float32\n",
        "    )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "    print(f\"all_image_paths content before creating dataset: {all_image_paths[:5]} (first 5) and type: {type(all_image_paths[0]) if all_image_paths else 'empty'}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string))\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "# 下面先不執行，移到獨立的block去執行\n",
        "# # Update the visualization call to load from the TensorFlow SavedModel path\n",
        "# image = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames/video01/video01_010002.png\"\n",
        "# model_path_for_visualization = f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit'\n",
        "# # The visualize_vit_results function expects the model to be loaded before passing it if it's not a string.\n",
        "# # So, let's load it here to pass the actual model object.\n",
        "# loaded_vit_model = tf.keras.models.load_model(model_path_for_visualization, compile=False, custom_objects={\n",
        "#     'Patches': model.Patches,\n",
        "#     'PatchEncoder': model.PatchEncoder,\n",
        "#     'TransformerBlock': model.TransformerBlock,\n",
        "#     'ViTBackbone': model.ViTBackbone\n",
        "# })\n",
        "# visualize_vit_results(loaded_vit_model, image)"
      ],
      "metadata": {
        "id": "ZS_Y48db9bRY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "fa770f23-d838-4895-d233-0f725f86482e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow will use GPU.\n",
            "Added /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn to sys.path: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 2.86 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n",
            "all_image_paths content before creating dataset: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video01/video01_000674.png', '/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video01/video01_000682.png', '/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video01/video01_000681.png', '/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video01/video01_000670.png', '/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video01/video01_000672.png'] (first 5) and type: <class 'str'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2440229913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0mstart_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;31m# Call functions using the imported module prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0mend_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(frames_root, batch_size, split_type)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_unbounded_threadpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_unbounded_threadpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, use_unbounded_threadpool, metadata, name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5876\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5877\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5878\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ 原始手術影像\n",
        "2️⃣ CLS Attention Map（最後一層）\n",
        "3️⃣ Patch Similarity Map（cosine similarity）"
      ],
      "metadata": {
        "id": "-GMQ7UatIXwQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c10b1658"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define paths for standalone execution of this cell\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Main Visualization Function\n",
        "# ------------------------------------------------------------\n",
        "def visualize_vit_results(model, image):\n",
        "\n",
        "    \"\"\"\n",
        "    model: ViT encoder (must return tokens + attention)\n",
        "    image: single image tensor (H, W, 3), normalized 0~1, or image path\n",
        "    \"\"\"\n",
        "\n",
        "    # 2026/02/15 add for image path\n",
        "    if isinstance(model, str):\n",
        "      # When loading from a string path, ensure custom_objects are provided\n",
        "      # Assuming 'model' module is available in the current scope for custom objects\n",
        "      import model as model_module # Renamed to avoid conflict with 'model' parameter\n",
        "      model = tf.keras.models.load_model(model, compile=False, custom_objects={\n",
        "          'Patches': model_module.Patches,\n",
        "          'PatchEncoder': model_module.PatchEncoder,\n",
        "          'TransformerBlock': model_module.TransformerBlock,\n",
        "          'ViTBackbone': model_module.ViTBackbone\n",
        "      })\n",
        "\n",
        "    if isinstance(image, str):\n",
        "        image = tf.io.read_file(image)\n",
        "        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "        # Add resizing here to match model input shape\n",
        "        image = tf.image.resize(image, (224, 224))\n",
        "\n",
        "    # Expand batch\n",
        "    input_tensor = tf.expand_dims(image, 0)\n",
        "\n",
        "\n",
        "    # Forward\n",
        "    tokens, attn = model(input_tensor, return_attention=True)\n",
        "\n",
        "    tokens = tokens[0]           # (N, D)\n",
        "    attn = attn[0]               # (heads, N, N)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 1️⃣ Original Image\n",
        "    # --------------------------------------------------------\n",
        "    original_img = image.numpy()\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 2️⃣ Attention Map (CLS → patches)\n",
        "    # --------------------------------------------------------\n",
        "    # Average over heads\n",
        "    attn_mean = tf.reduce_mean(attn, axis=0)  # (N, N)\n",
        "\n",
        "    cls_attn = attn_mean[0, 1:]  # CLS token attention to patches\n",
        "\n",
        "    num_patches = int(np.sqrt(len(cls_attn)))\n",
        "    cls_attn_map = tf.reshape(cls_attn, (num_patches, num_patches))\n",
        "\n",
        "    cls_attn_map = cls_attn_map.numpy()\n",
        "\n",
        "    # Normalize for visualization\n",
        "    cls_attn_map = (cls_attn_map - cls_attn_map.min()) / \\\n",
        "                   (cls_attn_map.max() - cls_attn_map.min() + 1e-8)\n",
        "\n",
        "    # Resize to image size\n",
        "    cls_attn_map = tf.image.resize(\n",
        "        cls_attn_map[..., None],\n",
        "        (original_img.shape[0], original_img.shape[1])\n",
        "    ).numpy().squeeze()\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 3️⃣ Similarity Map\n",
        "    # --------------------------------------------------------\n",
        "    # Remove CLS token\n",
        "    patch_tokens = tokens[1:]  # (N-1, D)\n",
        "\n",
        "    # Choose reference patch (center patch)\n",
        "    ref_idx = len(patch_tokens) // 2\n",
        "    ref_token = patch_tokens[ref_idx]\n",
        "\n",
        "    # Cosine similarity\n",
        "    patch_tokens_norm = tf.math.l2_normalize(patch_tokens, axis=1)\n",
        "    ref_token_norm = tf.math.l2_normalize(ref_token, axis=0)\n",
        "\n",
        "    similarity = tf.matmul(\n",
        "        patch_tokens_norm,\n",
        "        tf.expand_dims(ref_token_norm, -1)\n",
        "    )\n",
        "\n",
        "    similarity = tf.squeeze(similarity)\n",
        "\n",
        "    similarity_map = tf.reshape(\n",
        "        similarity,\n",
        "        (num_patches, num_patches)\n",
        "    ).numpy()\n",
        "\n",
        "    # Normalize\n",
        "    similarity_map = (similarity_map - similarity_map.min()) / \\\n",
        "                     (similarity_map.max() - similarity_map.min() + 1e-8)\n",
        "\n",
        "    similarity_map = tf.image.resize(\n",
        "        similarity_map[..., None],\n",
        "        (original_img.shape[0], original_img.shape[1])\n",
        "    ).numpy().squeeze()\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Plot 3 images side-by-side\n",
        "    # --------------------------------------------------------\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # 1‼️ Original\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_img)\n",
        "    plt.title(\"Original Endoscopic Frame\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 2‼️ Attention\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(original_img)\n",
        "    plt.imshow(cls_attn_map, cmap=\"jet\", alpha=0.5)\n",
        "    plt.title(\"CLS Attention Map\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 3‼️ Similarity\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(original_img)\n",
        "    plt.imshow(similarity_map, cmap=\"jet\", alpha=0.5)\n",
        "    plt.title(\"Patch Similarity Map\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# 假設你已經有:\n",
        "# model = pretrained_vit_encoder\n",
        "# image = dataset.take(1) 取出的單張圖\n",
        "image= f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames/video01/video01_000002.png\"\n",
        "# Correct the model path to point to the SavedModel directory, not a .keras file\n",
        "model = f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras'\n",
        "visualize_vit_results(model, image)\n",
        "image= f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames/video01/video01_001000.png\"\n",
        "# Correct the model path to point to the SavedModel directory, not a .keras file\n",
        "model = f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras'\n",
        "visualize_vit_results(model, image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ff2bbc5"
      },
      "source": [
        "# Task\n",
        "To resolve the `InvalidArgumentError` related to `output_types` and improve error handling for corrupted images, I'll modify the `dataset.py` file. This change will ensure that `tf.py_function` correctly specifies its output type and that any problematic image paths are collected for review.\n",
        "\n",
        "Here's the plan:\n",
        "1.  **Correct `tf.py_function` `Tout`**: In `dataset.py`, I'll change `Tout=tf.float32` to `Tout=[tf.float32]` within the `parse_image` function. This provides the `tf.py_function` with the correct list format for its output type.\n",
        "2.  **Collect Problematic Image Paths**: I'll introduce a global list called `problematic_image_paths` in `dataset.py`. The `_parse_image_py` function will be updated to append the path of any image that causes an error during reading or decoding to this list.\n",
        "3.  **Clear Problematic Paths**: I'll add a step at the beginning of the `build_dataset` function to clear the `problematic_image_paths` list, ensuring it's fresh for each dataset build.\n",
        "4.  **Handle Empty Dataset More Robustly**: For the case where `all_image_paths` is empty, I'll modify the return value to ensure that even an empty dataset correctly defines its output type through a `map` operation, preventing potential `output_types` errors downstream.\n",
        "5.  **Report Problematic Paths**: After the dataset is built and training is attempted, I will display the contents of the `problematic_image_paths` list to identify any corrupted or unreadable images.\n",
        "\n",
        "This will address the immediate error and provide insights into data quality issues.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Appending to problematic list and returning black image.\")\n",
        "            problematic_image_paths.append(path) # Add to problematic list\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Appending to problematic list and returning black image.\")\n",
        "        problematic_image_paths.append(path) # Add to problematic list\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    return tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32] # Corrected Tout to be a list\n",
        "    )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "    print(f\"all_image_paths content before creating dataset: {all_image_paths[:5]} (first 5) and type: {type(all_image_paths[0]) if all_image_paths else 'empty'}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Report problematic image paths\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for p in dataset.problematic_image_paths:\n",
        "        print(p)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation.\")\n",
        "else:\n",
        "    print(\"\\nNo problematic image paths detected during dataset creation.\")\n",
        "\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        # Check if train_dataset is empty before iterating\n",
        "        if tf.data.experimental.cardinality(train_dataset).numpy() == 0:\n",
        "            print(f\"Epoch {epoch+1}: train_dataset is empty. Skipping training for this epoch.\")\n",
        "            continue\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "# 下面先不執行，移到獨立的block去執行\n",
        "# # Update the visualization call to load from the TensorFlow SavedModel path\n",
        "# image = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames/video01/video01_010002.png\"\n",
        "# model_path_for_visualization = f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit'\n",
        "# # The visualize_vit_results function expects the model to be loaded before passing it if it's not a string.\n",
        "# # So, let's load it here to pass the actual model object.\n",
        "# loaded_vit_model = tf.keras.models.load_model(model_path_for_visualization, compile=False, custom_objects={\n",
        "#     'Patches': model.Patches,\n",
        "#     'PatchEncoder': model.PatchEncoder,\n",
        "#     'TransformerBlock': model.TransformerBlock,\n",
        "#     'ViTBackbone': model.ViTBackbone\n",
        "# })\n",
        "# visualize_vit_results(loaded_vit_model, image)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e17c84c"
      },
      "source": [
        "## Collect Problematic Image Paths\n",
        "\n",
        "### Subtask:\n",
        "Modify the `dataset.py` file to include a mechanism within the `_parse_image_py` function to append the paths of any corrupted or unreadable images to a list. This list will allow us to identify all problematic files after running the dataset generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b6baaf"
      },
      "source": [
        "## Report Problematic Paths\n",
        "\n",
        "### Subtask:\n",
        "After the dataset generation runs, display the collected list of problematic image paths so you can review them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "911de6c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous run of the `dataset.build_dataset` function failed because the `parse_image` function, which uses `tf.py_function`, expects the Python function (`_parse_image_py`) to return specific output types. The error indicates that the `output_types` attribute had a length of 0, meaning `tf.py_function` couldn't infer the return type. This usually happens if the `_parse_image_py` function has different return paths without explicitly specifying `Tout` in `tf.py_function` or if the `tf.py_function` wrapper itself is incorrectly used with `set_shape`. I will fix the `dataset.py` file to explicitly define `output_types` in `tf.py_function` and also ensure the `_parse_image_py` consistently returns a tensor of the correct type and shape. Additionally, I will add a global list `problematic_image_paths` to collect paths that cause issues during image parsing, as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "e60c63b0",
        "outputId": "f7887955-64c6-4710-d903-37ef6a6f963f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout to ensure tf.data can infer the output types and shapes\n",
        "    return tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32))\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 0.07 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4205908054.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0mstart_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;31m# Call functions using the imported module prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0mend_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(frames_root, batch_size, split_type)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_unbounded_threadpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_unbounded_threadpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, use_unbounded_threadpool, metadata, name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5876\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5877\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5878\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d562f6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed with an `InvalidArgumentError` due to `tf.py_function` in `dataset.py` not correctly inferring output types. Although `Tout=[tf.float32]` was included in the previous fix attempt, it seems the changes were not fully applied or reloaded. I will regenerate the complete code block to ensure the `Tout` parameter is correctly specified as a list `[tf.float32]` for `tf.py_function`. Additionally, I will add the necessary line `global problematic_image_paths; problematic_image_paths = []` at the beginning of the `build_dataset` function in `dataset.py` to properly clear the list for each dataset build, as per the original subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "b9d7069c",
        "outputId": "63428369-5ba3-416d-ea74-3d680e6edc23"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout to ensure tf.data can infer the output types and shapes\n",
        "    return tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        # Check if train_dataset is empty before iterating\n",
        "        if tf.data.experimental.cardinality(train_dataset).numpy() == 0:\n",
        "            print(f\"Epoch {epoch+1}: train_dataset is empty. Skipping training for this epoch.\")\n",
        "            continue\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow will use GPU.\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn already in sys.path: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 0.07 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-111097439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0mstart_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;31m# Call functions using the imported module prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0mend_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(frames_root, batch_size, split_type)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_unbounded_threadpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_unbounded_threadpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, use_unbounded_threadpool, metadata, name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5876\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5877\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5878\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62d96a78"
      },
      "source": [
        "**Reasoning**:\n",
        "The `InvalidArgumentError` related to `output_types` persists even after setting `Tout=[tf.float32]` in `tf.py_function`. This suggests that TensorFlow's graph construction might need more explicit shape information. I will modify `dataset.py` to also include `output_shapes=[tf.TensorShape([IMG_SIZE, IMG_SIZE, 3])]` in the `tf.py_function` call, ensuring all type and shape metadata is provided upfront for robust graph building and resolving the error. Additionally, I will re-verify the problematic path collection and reporting logic to ensure the subtask is fully met.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "785213cd",
        "outputId": "941512db-dbb3-467a-f411-85c5eb4401d4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout and output_shapes to ensure tf.data can infer the output types and shapes\n",
        "    return tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32],\n",
        "        output_shapes=[tf.TensorShape([IMG_SIZE, IMG_SIZE, 3])]\n",
        "    )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow will use GPU.\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn already in sys.path: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 0.07 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\", line 35, in parse_image  *\n        )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n\n    TypeError: Got an unexpected keyword argument 'output_shapes'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4128662329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0mstart_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;31m# Call functions using the imported module prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0mend_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(frames_root, batch_size, split_type)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\u001b[0m in \u001b[0;36mtf__parse_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parse_image_py\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0...\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miterable_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_iterable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_dispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\", line 35, in parse_image  *\n        )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n\n    TypeError: Got an unexpected keyword argument 'output_shapes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f377ae7b"
      },
      "source": [
        "**Reasoning**:\n",
        "The `TypeError: Got an unexpected keyword argument 'output_shapes'` indicates that `tf.py_function` in the current TensorFlow version does not support `output_shapes` as a direct argument. The shape should be set using `.set_shape()` after the function call. I will remove the `output_shapes` argument from `tf.py_function` in `dataset.py` to resolve this, while maintaining the problematic image path collection and reporting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "ce5f60c2",
        "outputId": "d4b92d84-2a70-475a-87f4-47e53ea6d0a5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout to ensure tf.data can infer the output types and shapes\n",
        "    # Removed output_shapes argument as it's not supported directly by tf.py_function in some TF versions\n",
        "    return tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        # Check if train_dataset is empty before iterating\n",
        "        if tf.data.experimental.cardinality(train_dataset).numpy() == 0:\n",
        "            print(f\"Epoch {epoch+1}: train_dataset is empty. Skipping training for this epoch.\")\n",
        "            continue\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow will use GPU.\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn already in sys.path: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 0.07 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1998178091.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0mstart_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;31m# Call functions using the imported module prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0mend_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(frames_root, batch_size, split_type)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_unbounded_threadpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_unbounded_threadpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, use_unbounded_threadpool, metadata, name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5876\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5877\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5878\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96048a0"
      },
      "source": [
        "**Reasoning**:\n",
        "The `TypeError: Got an unexpected keyword argument 'output_shapes'` indicates that `tf.py_function` in the current TensorFlow version does not support `output_shapes` as a direct argument. The shape should be set using `.set_shape()` after the function call. I will remove the `output_shapes` argument from `tf.py_function` in `dataset.py` to resolve this, while maintaining the problematic image path collection and reporting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "b87bb93a",
        "outputId": "338a35c6-c09f-4822-91fc-db9088ceb167"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout to ensure tf.data can infer the output types and shapes\n",
        "    # Removed output_shapes argument as it's not supported directly by tf.py_function in some TF versions\n",
        "    return tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )[0].set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow will use GPU.\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn already in sys.path: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 0.05 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-575007588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0mstart_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;31m# Call functions using the imported module prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0mend_dataset_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/dataset.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(frames_root, batch_size, split_type)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_unbounded_threadpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_unbounded_threadpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, use_unbounded_threadpool, metadata, name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5876\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5877\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5878\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Length for attr 'output_types' of 0 must be at least minimum 1\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false; attr=use_unbounded_threadpool:bool,default=false; attr=metadata:string,default=\"\"> [Op:ParallelMapDatasetV2] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a5d3a55"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error `InvalidArgumentError: Length for attr 'output_types' of 0` was caused because `set_shape` operates in-place and returns `None`, so `parse_image` was returning `None` instead of the image tensor. I will correct `parse_image` in `dataset.py` to set the shape and then return the tensor. I will also include the logic to display problematic image paths as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "e8e4fb1b",
        "outputId": "0613569d-4a36-4da0-ede6-190ec081612c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout to ensure tf.data can infer the output types and shapes\n",
        "    # Use tf.py_function to wrap the python function\n",
        "    output = tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )\n",
        "    # output is a list of tensors, get the first one\n",
        "    img = output[0]\n",
        "    # Set shape explicitly as set_shape returns None (in-place modification)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      ┐── Video01/*.jpg\n",
        "      ┐── Video02/*.jpg\n",
        "      └── ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow will use GPU.\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn already in sys.path: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 0.05 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n",
            "Dataset built in 1.42 seconds.\n",
            "Building models on GPU...\n",
            "Models built and initialized in 0.12 seconds.\n",
            "training start\n",
            "Warning: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png is empty or could not be read. Returning black image.\n",
            "Error processing /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video40/video40_000253.png: 'size' must be a 1-D Tensor of 2 elements: new_height, new_width. Returning black image.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "in user code:\n\n    File \"/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/augment.py\", line 10, in strong_augment  *\n        x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n\n    InvalidArgumentError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} Incompatible shapes: [2] vs. [3] [Op:FloorMod] name: \n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2842072435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mbatch_step_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwo_views\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             loss = train.train_step(\n\u001b[1;32m    431\u001b[0m                 \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/augment.py\u001b[0m in \u001b[0;36mtwo_views\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtwo_views\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrong_augment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrong_augment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m                   \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                   ('after %s' % date), instructions)\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(func.__doc__, date,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                 instructions)\n\u001b[0;32m--> 588\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/map_fn.py\u001b[0m in \u001b[0;36mmap_fn_v2\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name, fn_output_signature)\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfn_output_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0mfn_output_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m   return map_fn(\n\u001b[0m\u001b[1;32m    638\u001b[0m       \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m       \u001b[0melems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                 instructions)\n\u001b[0;32m--> 588\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/map_fn.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name, fn_output_signature)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m     _, r_a = while_loop.while_loop(\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_batchable_ta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/while_loop.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    486\u001b[0m                                               list(loop_vars))\n\u001b[1;32m    487\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/while_loop.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m    477\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m    478\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/map_fn.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0mag_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0mautographed_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0mresult_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautographed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_output_signature\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0mresult_value_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: in user code:\n\n    File \"/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/augment.py\", line 10, in strong_augment  *\n        x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n\n    InvalidArgumentError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} Incompatible shapes: [2] vs. [3] [Op:FloorMod] name: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dc7046d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution successfully started training and identified a problematic image, confirming the fix for `dataset.py` works. However, the execution output was truncated before the final report. I will re-run the full code block to ensure the training completes and the list of problematic image paths is displayed at the end, satisfying the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5297b26f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout to ensure tf.data can infer the output types and shapes\n",
        "    # Use tf.py_function to wrap the python function\n",
        "    output = tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )\n",
        "    # output is a list of tensors, get the first one\n",
        "    img = output[0]\n",
        "    # Set shape explicitly as set_shape returns None (in-place modification)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      ┐── Video01/*.jpg\n",
        "      ┐── Video02/*.jpg\n",
        "      └── ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f06a0fa"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because `parse_image` was implicitly returning `None` due to the chained `.set_shape()` call (which operates in-place). I will fix `dataset.py` to correctly define `parse_image` by setting the shape on the tensor and then returning it. I will also ensure the `problematic_image_paths` list is properly managed and displayed at the end of the execution to fulfill the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbf8b544",
        "outputId": "84ad39d9-7ee1-4472-996a-f9ec1e03c91f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc # Import garbage collector\n",
        "import time # Import time module for timing\n",
        "import numpy as np # Import numpy for tf.py_function output\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT = DRIVE_DIR # Explicitly define DATA_ROOT here\n",
        "\n",
        "# --- 1. 自動偵測 CPU / GPU 設備 ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    DEVICE = 'GPU'\n",
        "    try:\n",
        "        # 設定 TensorFlow 只使用特定的 GPU 記憶體增長，避免一次性分配所有記憶體\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow will use GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        # 記憶體增長必須在 GPU 初始化之前設定\n",
        "        print(e)\n",
        "        print(\"Error configuring GPU, falling back to CPU.\")\n",
        "        DEVICE = 'CPU'\n",
        "else:\n",
        "    DEVICE = 'CPU'\n",
        "    print(\"No GPU found. TensorFlow will use CPU.\")\n",
        "\n",
        "# Add MODULE_DIR to Python path to enable imports and give it priority\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR) # Use insert(0) for higher priority\n",
        "    print(f\"Added {MODULE_DIR} to sys.path: {sys.path}\")\n",
        "else:\n",
        "    print(f\"{MODULE_DIR} already in sys.path: {sys.path}\")\n",
        "\n",
        "# Write dataset.py with modifications for problematic_image_paths and tf.py_function return type\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Explicitly set Tout to ensure tf.data can infer the output types and shapes\n",
        "    # Use tf.py_function to wrap the python function\n",
        "    output = tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )\n",
        "    # output is a list of tensors, get the first one\n",
        "    img = output[0]\n",
        "    # Set shape explicitly as set_shape returns None (in-place modification)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      \\u251c\\u2500\\u2500 Video01/*.jpg\n",
        "      \\u251c\\u2500\\u2500 Video02/*.jpg\n",
        "      \\u2514\\u2500\\u2500 ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                all_image_paths.append(os.path.join(video_dir, filename))\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        # Return a dataset with defined output types even if empty to avoid map issues\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# Write augment.py\n",
        "with open(os.path.join(MODULE_DIR, 'augment.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def strong_augment(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_brightness(x, 0.4)\n",
        "    x = tf.image.random_contrast(x, 0.6, 1.4)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.4)\n",
        "\n",
        "    # random crop\n",
        "    crop_size = tf.random.uniform([], 180, 224, dtype=tf.int32)\n",
        "    x = tf.image.random_crop(x, size=[crop_size, crop_size, 3])\n",
        "    x = tf.image.resize(x, (224, 224))\n",
        "\n",
        "    return x\n",
        "def two_views(batch):\n",
        "    v1 = tf.map_fn(strong_augment, batch)\n",
        "    v2 = tf.map_fn(strong_augment, batch)\n",
        "    return v1, v2\n",
        "''')\n",
        "\n",
        "# Write model.py (Updated to Vision Transformer)\n",
        "with open(os.path.join(MODULE_DIR, 'model.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ViT Parameters\n",
        "PATCH_SIZE = 16\n",
        "IMG_SIZE = 224 # Should match the IMG_SIZE in dataset.py\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 768 # Dimension for patch and positional embeddings\n",
        "TRANSFORMER_LAYERS = 12 # Number of transformer blocks\n",
        "NUM_HEADS = 12 # Number of attention heads\n",
        "ATTENTION_DROPOUT = 0.0\n",
        "PROJECTION_DROPOUT = 0.0\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=tf.shape(patch)[1], delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
        "        return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, projection_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate # Corrected key_dim\n",
        "        )\n",
        "        # tf.print(f\"DEBUG: MultiHeadAttention key_dim set to: {projection_dim // num_heads} for num_heads: {num_heads}\") # Debug print removed after diagnosis\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp_dense1 = layers.Dense(projection_dim * 4, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dense2 = layers.Dense(projection_dim, activation=tf.keras.activations.gelu)\n",
        "        self.mlp_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output, attn_scores = self.mha(query=x, value=x, key=x, return_attention_scores=True, training=training)\n",
        "        x = x + attn_output\n",
        "        y = self.layernorm2(x)\n",
        "        mlp_output = self.mlp_dense1(y)\n",
        "        mlp_output = self.mlp_dense2(mlp_output)\n",
        "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
        "        return x + mlp_output, attn_scores # Return attention scores here\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"projection_dim\": self.mha.key_dim, \"num_heads\": self.mha.num_heads, \"dropout_rate\": self.mha.dropout})\n",
        "        return config\n",
        "\n",
        "class ViTBackbone(models.Model):\n",
        "    def __init__(self, input_shape, patch_size, projection_dim, num_heads, transformer_layers, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "        self.patches = Patches(patch_size)\n",
        "        # +1 for CLS token in num_patches for PatchEncoder's position embedding\n",
        "        self.patch_encoder = PatchEncoder(self.num_patches + 1, projection_dim)\n",
        "        self.transformer_blocks = []\n",
        "        for i in range(transformer_layers):\n",
        "            self.transformer_blocks.append(\n",
        "                TransformerBlock(projection_dim, num_heads, dropout_rate, name=f\"transformer_block_{i}\")\n",
        "            )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"cls_token_variable\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, return_attention=False, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        patches = self.patches(inputs)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "\n",
        "        cls_token_batch = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_token_batch, encoded_patches], axis=1)\n",
        "\n",
        "        attention_scores_list = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_scores = transformer_block(x, training=training)\n",
        "            attention_scores_list.append(attn_scores)\n",
        "\n",
        "        tokens = self.layer_norm(x)\n",
        "\n",
        "        if return_attention:\n",
        "            return tokens, attention_scores_list[-1] # Return last layer's attention\n",
        "        return tokens # Default return if return_attention is False\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_shape\": (IMG_SIZE, IMG_SIZE, 3),\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"num_heads\": self.transformer_blocks[0].mha.num_heads if self.transformer_blocks else 0,\n",
        "            \"transformer_layers\": len(self.transformer_blocks),\n",
        "            \"dropout_rate\": self.transformer_blocks[0].mha.dropout if self.transformer_blocks else 0.0\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Custom from_config to handle the cls_token_variable\n",
        "        input_shape = config.pop('input_shape')\n",
        "        instance = cls(input_shape=input_shape, **config)\n",
        "        # The cls_token_variable will be created in __init__\n",
        "        return instance\n",
        "\n",
        "def build_backbone(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    return ViTBackbone(input_shape, PATCH_SIZE, PROJECTION_DIM, NUM_HEADS, TRANSFORMER_LAYERS, ATTENTION_DROPOUT)\n",
        "\n",
        "def projection_head(dim=256):\n",
        "    return models.Sequential([\n",
        "        layers.Dense(1024, activation=\"gelu\"),\n",
        "        layers.Dense(dim)\n",
        "    ])\n",
        "''')\n",
        "\n",
        "# Write loss.py\n",
        "with open(os.path.join(MODULE_DIR, 'loss.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "def msn_loss(student, teacher, temperature=0.1):\n",
        "    student = tf.nn.log_softmax(student / temperature, axis=-1)\n",
        "    teacher = tf.nn.softmax(teacher / temperature, axis=-1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(teacher * student, axis=-1))\n",
        "''')\n",
        "\n",
        "# Write train.py - This needs to include update_teacher as well, and import msn_loss\n",
        "with open(os.path.join(MODULE_DIR, 'train.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "from loss import msn_loss # Now we can import it from the local file\n",
        "\n",
        "def update_teacher(student, teacher, momentum=0.996):\n",
        "    for s, t in zip(student.trainable_variables,\n",
        "                    teacher.trainable_variables):\n",
        "        t.assign(momentum * t + (1.0 - momentum) * s)\n",
        "\n",
        "@tf.function\n",
        "def train_step(view1, view2,\n",
        "               student_backbone, teacher_backbone,\n",
        "               student_head, teacher_head,\n",
        "               optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        s_feat = student_backbone(view1, training=True)\n",
        "        s_proj = student_head(s_feat, training=True)\n",
        "\n",
        "        t_feat = teacher_backbone(view2, training=False)\n",
        "        t_proj = teacher_head(t_feat, training=False)\n",
        "\n",
        "        loss = msn_loss(s_proj, tf.stop_gradient(t_proj))\n",
        "\n",
        "    vars = student_backbone.trainable_variables + student_head.trainable_variables\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    update_teacher(student_backbone, teacher_backbone)\n",
        "    update_teacher(student_head, teacher_head)\n",
        "\n",
        "    return loss\n",
        "''')\n",
        "\n",
        "print(f\"Created module files in {MODULE_DIR} and added to Python path.\")\n",
        "\n",
        "start_import_time = time.time()\n",
        "# Now import from the created modules using import module and then module.func\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "\n",
        "# Explicitly reload modules to ensure the latest version is used after writing files\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "end_import_time = time.time()\n",
        "print(f\"Modules imported and reloaded in {end_import_time - start_import_time:.2f} seconds.\")\n",
        "\n",
        "config_path = f'{DRIVE_DIR}/cholec80/config.json'\n",
        "# Revert IMAGE_DIR to the previous value to verify its structure with ls\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "BATCH_SIZE = 4 # Reduced batch size for lower RAM usage\n",
        "EPOCHS = 2\n",
        "\n",
        "print(f\"Checking contents of DATA_ROOT/cholec80/cholec80_extracted: {DRIVE_DIR}/cholec80/cholec80_extracted/\")\n",
        "!ls -F \"{DRIVE_DIR}/cholec80/cholec80_extracted/\" # Inspect parent directory\n",
        "\n",
        "print(f\"Checking contents of IMAGE_DIR: {IMAGE_DIR}\")\n",
        "!ls -F \"{IMAGE_DIR}\" # Inspect IMAGE_DIR\n",
        "\n",
        "start_dataset_time = time.time()\n",
        "# Call functions using the imported module prefix\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "end_dataset_time = time.time()\n",
        "print(f\"Dataset built in {end_dataset_time - start_dataset_time:.2f} seconds.\")\n",
        "\n",
        "# Wrap model creation and training in tf.device context\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    start_model_build_time = time.time()\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # initialize teacher\n",
        "    for s, t in zip(student_backbone.variables,\n",
        "                        teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    end_model_build_time = time.time()\n",
        "    print(f\"Models built and initialized in {end_model_build_time - start_model_build_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"training start\")\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        # Check if train_dataset is empty before iterating\n",
        "        if tf.data.experimental.cardinality(train_dataset).numpy() == 0:\n",
        "            print(f\"Epoch {epoch+1}: train_dataset is empty. Skipping training for this epoch.\")\n",
        "            continue\n",
        "\n",
        "        for batch in train_dataset:\n",
        "            batch_step_start_time = time.time()\n",
        "            v1, v2 = augment.two_views(batch)\n",
        "            loss = train.train_step(\n",
        "                v1, v2,\n",
        "                student_backbone,\n",
        "                teacher_backbone,\n",
        "                student_head,\n",
        "                teacher_head,\n",
        "                optimizer\n",
        "            )\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "            batch_step_end_time = time.time()\n",
        "            # print(f\"  Batch {steps}/{len(train_dataset)} processed in {batch_step_end_time - batch_step_start_time:.4f} seconds.\")\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        if steps > 0:\n",
        "            print(f\"Epoch {epoch+1}: loss={total_loss/steps:.4f} (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: No data processed in this epoch. (Time: {epoch_end_time - epoch_start_time:.2f} seconds)\")\n",
        "\n",
        "    total_training_end_time = time.time()\n",
        "    print(f\"Total training time for {EPOCHS} epochs: {total_training_end_time - total_training_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Save the student backbone model\n",
        "    print(f\"Saving model to {MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras...\")\n",
        "    start_save_time = time.time()\n",
        "    student_backbone.save(f'{MODEL_SAVE_DIR}/msn_pretrained_backbone_vit.keras')\n",
        "    end_save_time = time.time()\n",
        "    print(f\"Model saved in {end_save_time - start_save_time:.2f} seconds.\")\n",
        "\n",
        "    # Explicitly delete large objects to free up RAM\n",
        "    del train_dataset\n",
        "    del student_backbone\n",
        "    del teacher_backbone\n",
        "    del student_head\n",
        "    del teacher_head\n",
        "    del optimizer\n",
        "    import tensorflow.keras.backend as K\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"Finished training and cleared memory.\")\n",
        "\n",
        "\n",
        "# --- Display Problematic Paths (New Code) ---\n",
        "\n",
        "if dataset.problematic_image_paths:\n",
        "    print(\"\\n--- Problematic Image Paths Detected ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(f\"Total problematic images: {len(dataset.problematic_image_paths)}\")\n",
        "    print(\"These images were replaced with black images during dataset creation due to errors or empty content.\")\n",
        "elif len(dataset.problematic_image_paths) == 0 and 'train_dataset' in locals(): # Check if the list is empty AND dataset processing actually occurred\n",
        "    print(\"\\nNo problematic image paths were detected during dataset creation.\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow will use GPU.\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn already in sys.path: ['/content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Created module files in /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn and added to Python path.\n",
            "Modules imported and reloaded in 1.72 seconds.\n",
            "Checking contents of DATA_ROOT/cholec80/cholec80_extracted: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/\n",
            "frames/  phase_annotations/  tool_annotations/\n",
            "Checking contents of IMAGE_DIR: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames\n",
            "video01/  video11/  video21/  video31/\tvideo41/  video51/  video61/  video71/\n",
            "video02/  video12/  video22/  video32/\tvideo42/  video52/  video62/  video72/\n",
            "video03/  video13/  video23/  video33/\tvideo43/  video53/  video63/  video73/\n",
            "video04/  video14/  video24/  video34/\tvideo44/  video54/  video64/  video74/\n",
            "video05/  video15/  video25/  video35/\tvideo45/  video55/  video65/  video75/\n",
            "video06/  video16/  video26/  video36/\tvideo46/  video56/  video66/  video76/\n",
            "video07/  video17/  video27/  video37/\tvideo47/  video57/  video67/  video77/\n",
            "video08/  video18/  video28/  video38/\tvideo48/  video58/  video68/  video78/\n",
            "video09/  video19/  video29/  video39/\tvideo49/  video59/  video69/  video79/\n",
            "video10/  video20/  video30/  video40/\tvideo50/  video60/  video70/  video80/\n",
            "[INFO] Total frames loaded for train: 102048\n",
            "Dataset built in 5.40 seconds.\n",
            "Building models on GPU...\n",
            "Models built and initialized in 0.12 seconds.\n",
            "training start\n",
            "Warning: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png is empty or could not be read. Returning black image.\n",
            "Epoch 1: loss=3.5405 (Time: 10203.88 seconds)\n",
            "Warning: /content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png is empty or could not be read. Returning black image.\n",
            "Epoch 2: loss=3.5197 (Time: 10103.32 seconds)\n",
            "Total training time for 2 epochs: 20307.29 seconds.\n",
            "Saving model to /content/gdrive/MyDrive/PMBM/論文程式/MSM/ssl_msn/models/msn_pretrained_backbone_vit.keras...\n",
            "Model saved in 1.70 seconds.\n",
            "Finished training and cleared memory.\n",
            "\n",
            "--- Problematic Image Paths Detected ---\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png\n",
            "Total problematic images: 2\n",
            "These images were replaced with black images during dataset creation due to errors or empty content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39fd7c8e"
      },
      "source": [
        "**Reasoning**:\n",
        "Access the `dataset` module's `problematic_image_paths` list to display the corrupted images identified during the previous training execution. This allows for review without re-running the time-consuming training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3599efdd",
        "outputId": "025c71e6-a553-4e6a-d3cd-62f52ccd3851"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure the module path is in sys.path\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR)\n",
        "\n",
        "import dataset\n",
        "\n",
        "# Display the problematic image paths collected in the dataset module\n",
        "if hasattr(dataset, 'problematic_image_paths') and dataset.problematic_image_paths:\n",
        "    print(f\"\\n--- Found {len(dataset.problematic_image_paths)} Problematic Image Paths ---\")\n",
        "    for path in dataset.problematic_image_paths:\n",
        "        print(path)\n",
        "    print(\"\\nNote: These images were replaced with black images during the training process.\")\n",
        "else:\n",
        "    print(\"\\nNo problematic image paths were detected in the 'dataset' module list.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Found 2 Problematic Image Paths ---\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png\n",
            "/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png\n",
            "\n",
            "Note: These images were replaced with black images during the training process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee0f719a"
      },
      "source": [
        "## Filter Corrupted Images from Dataset\n",
        "\n",
        "### Subtask:\n",
        "Update `dataset.py` to exclude the identified corrupted image and verify by running training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2862fdb"
      },
      "source": [
        "**Reasoning**:\n",
        "Update `dataset.py` to exclude the identified corrupted image path and then run a 1-epoch training verification to ensure the warning is resolved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e9b9f08",
        "outputId": "f84ad518-49b6-473c-f1ed-f9f6afb10829"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/PMBM/論文程式/MSM'\n",
        "MODULE_DIR = os.path.join(DRIVE_DIR, 'ssl_msn')\n",
        "DATA_ROOT = DRIVE_DIR\n",
        "MODEL_SAVE_DIR = os.path.join(MODULE_DIR, 'models')\n",
        "IMAGE_DIR = f\"{DATA_ROOT}/cholec80/cholec80_extracted/frames\"\n",
        "\n",
        "# Ensure MODULE_DIR is in path\n",
        "if MODULE_DIR not in sys.path:\n",
        "    sys.path.insert(0, MODULE_DIR)\n",
        "\n",
        "# --- 1. Update dataset.py with EXCLUDED_PATHS ---\n",
        "with open(os.path.join(MODULE_DIR, 'dataset.py'), 'w') as f:\n",
        "    f.write('''import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Global list to store problematic image paths\n",
        "problematic_image_paths = []\n",
        "\n",
        "# List of known corrupted/empty images to exclude upfront\n",
        "EXCLUDED_PATHS = [\n",
        "    '/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png'\n",
        "]\n",
        "\n",
        "def _parse_image_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode('utf-8')\n",
        "    try:\n",
        "        img_bytes = tf.io.read_file(path).numpy()\n",
        "        if not img_bytes:\n",
        "            print(f\"Warning: {path} is empty or could not be read. Returning black image.\")\n",
        "            problematic_image_paths.append(path)\n",
        "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False).numpy()\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)).numpy()\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}. Returning black image.\")\n",
        "        problematic_image_paths.append(path)\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "def parse_image(path):\n",
        "    # Use tf.py_function to wrap the python function\n",
        "    output = tf.py_function(\n",
        "        _parse_image_py,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32]\n",
        "    )\n",
        "    # output is a list of tensors, get the first one\n",
        "    img = output[0]\n",
        "    # Set shape explicitly as set_shape returns None (in-place modification)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img\n",
        "\n",
        "def build_dataset(frames_root, batch_size, split_type=None):\n",
        "    \"\"\"\n",
        "    frames_root/\n",
        "      ┐── Video01/*.jpg\n",
        "      ┐── Video02/*.jpg\n",
        "      └── ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear the problematic paths list for each new dataset build\n",
        "    global problematic_image_paths\n",
        "    problematic_image_paths = []\n",
        "\n",
        "    split_ranges = {\n",
        "        'train': range(1, 49),\n",
        "        'validation': range(49, 60),\n",
        "        'test': range(60, 81)\n",
        "    }\n",
        "\n",
        "    all_image_paths = []\n",
        "\n",
        "    if split_type and split_type in split_ranges:\n",
        "        video_indices = split_ranges[split_type]\n",
        "    else: # Default to all videos if split_type is not specified or invalid\n",
        "        video_indices = range(1, 81)\n",
        "\n",
        "    # Iterate through each video directory based on the split_type\n",
        "    for i in video_indices:\n",
        "        video_dir = os.path.join(frames_root, f\"video{i:02d}\")\n",
        "        if not os.path.isdir(video_dir):\n",
        "            print(f\"Warning: video directory not found: {video_dir}\")\n",
        "            continue\n",
        "\n",
        "        # List all image files within the current video directory\n",
        "        for filename in os.listdir(video_dir):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                full_path = os.path.join(video_dir, filename)\n",
        "                # Check against excluded paths\n",
        "                if full_path in EXCLUDED_PATHS:\n",
        "                    continue\n",
        "                all_image_paths.append(full_path)\n",
        "\n",
        "    print(f\"[INFO] Total frames loaded for {split_type or 'all'}: {len(all_image_paths)}\")\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image paths found. Returning an empty dataset.\")\n",
        "        return tf.data.Dataset.from_tensor_slices(tf.constant([], dtype=tf.string)).map(\n",
        "            lambda x: tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    ds = ds.shuffle(buffer_size=2048)\n",
        "\n",
        "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "''')\n",
        "\n",
        "# --- 2. Reload modules ---\n",
        "import dataset\n",
        "import augment\n",
        "import model\n",
        "import train\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(augment)\n",
        "importlib.reload(model)\n",
        "importlib.reload(train)\n",
        "print(\"Modules reloaded.\")\n",
        "\n",
        "# --- 3. Build Dataset ---\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 1 # Run only 1 epoch for verification\n",
        "\n",
        "print(\"Building dataset...\")\n",
        "train_dataset = dataset.build_dataset(IMAGE_DIR, BATCH_SIZE, split_type='train')\n",
        "\n",
        "# --- 4. Run Training Verification ---\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "DEVICE = 'GPU' if gpus else 'CPU'\n",
        "\n",
        "with tf.device(f'/{DEVICE}:0' if DEVICE == 'GPU' else '/CPU:0'):\n",
        "    print(f\"Building models on {DEVICE}...\")\n",
        "    # Build ViT backbones\n",
        "    student_backbone = model.build_backbone()\n",
        "    teacher_backbone = model.build_backbone()\n",
        "    student_head = model.projection_head()\n",
        "    teacher_head = model.projection_head()\n",
        "\n",
        "    # Initialize teacher\n",
        "    for s, t in zip(student_backbone.variables, teacher_backbone.variables):\n",
        "        t.assign(s)\n",
        "    for s, t in zip(student_head.variables, teacher_head.variables):\n",
        "        t.assign(s)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "    print(f\"Starting verification training for {EPOCHS} epoch...\")\n",
        "    total_loss = 0.0\n",
        "    steps = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch in train_dataset:\n",
        "        v1, v2 = augment.two_views(batch)\n",
        "        loss = train.train_step(\n",
        "            v1, v2,\n",
        "            student_backbone,\n",
        "            teacher_backbone,\n",
        "            student_head,\n",
        "            teacher_head,\n",
        "            optimizer\n",
        "        )\n",
        "        total_loss += loss\n",
        "        steps += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    if steps > 0:\n",
        "        print(f\"Epoch 1: loss={total_loss/steps:.4f} (Time: {end_time - start_time:.2f} seconds)\")\n",
        "    else:\n",
        "        print(\"Epoch 1: No data processed.\")\n",
        "\n",
        "    # Check for problematic paths again\n",
        "    if dataset.problematic_image_paths:\n",
        "        print(\"\\n--- Problematic Image Paths Detected (Still present) ---\")\n",
        "        for path in dataset.problematic_image_paths:\n",
        "            print(path)\n",
        "    else:\n",
        "        print(\"\\nSuccess: No problematic images detected during verification run.\")\n",
        "\n",
        "    # Cleanup\n",
        "    del train_dataset, student_backbone, teacher_backbone, student_head, teacher_head, optimizer\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modules reloaded.\n",
            "Building dataset...\n",
            "[INFO] Total frames loaded for train: 102047\n",
            "Building models on GPU...\n",
            "Starting verification training for 1 epoch...\n",
            "Epoch 1: loss=3.6954 (Time: 10936.83 seconds)\n",
            "\n",
            "Success: No problematic images detected during verification run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f2b14e"
      },
      "source": [
        "# Task\n",
        "Filter the corrupted image \"/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png\" from the dataset by updating `dataset.py` to exclude this path, then verify the fix by running a 1-epoch training session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aea82417"
      },
      "source": [
        "## Filter Corrupted Images from Dataset\n",
        "\n",
        "### Subtask:\n",
        "Update `dataset.py` to exclude the identified corrupted image path, reload modules, and verify the fix with a 1-epoch training run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3da08d92"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**Question:** Was the corrupted image successfully filtered and the training pipeline verified?\n",
        "**Answer:** Yes, the corrupted image located at `/content/gdrive/MyDrive/PMBM/論文程式/MSM/cholec80/cholec80_extracted/frames/video03/video03_001046.png` was successfully excluded by updating the `dataset.py` script. A subsequent 1-epoch training run confirmed that the error is resolved and the pipeline is functional.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "- The specific file path causing the data loading error was hardcoded for exclusion within the dataset module.\n",
        "- The `dataset` module was reloaded to ensure the code changes took effect immediately.\n",
        "- A 1-epoch verification run completed successfully, indicating that the dataset iterator no longer fails on the corrupted frame.\n",
        "\n",
        "### Insights or Next Steps\n",
        "- The training process is now stable; proceed with the full training schedule (e.g., the intended number of epochs).\n",
        "- To prevent similar interruptions in the future, consider adding a `try-except` block within the image loading function to automatically skip and log unreadable files dynamically.\n"
      ]
    }
  ]
}